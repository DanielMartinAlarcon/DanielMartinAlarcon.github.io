---
title: Simpson Says
subtitle: Natural Language Processing and web app deployment
image: /img/8_simpson/homer.png
---

**TL;DR**: I used simple NLP to write a search function that will compare a user query to each line of dialogue in ~600 episodes of The Simpsons, and deployed it in a live web application. This was a one-week team project with other data scientists and web developers at Lambda School, and resulted in [a fully functional website](https://simpsonssays.netlify.com/) where you can access our models yourself. The full repo for this project is [here](https://github.com/simpson-says/buildweek3-simpsons-says-ds).

# Using text similarity to find iconic quotes.
The full script for about 27 seasons and 600 episodes of the show are available [on Kaggle](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data). A relatively small amount of processing is needed to create a semantic index of all the existing quotes, to which new quotes can be compared for similarities.  We did that here using the Natural Language Toolkit ([NLTK](https://www.nltk.org/)) and the semantic analysis tool [Gensim](https://radimrehurek.com/gensim/). 

The basic steps for text processing are:  

1. **Clean the text.**  I made a pandas dataframe where the column `normalized_text` contains all the lines of dialog for all the episodes, stripped of punctuation and turned to lowercase.  
2. **Tokenize.** Each row was a string with a single line of dialog.  I split that string into a list of individual words. 
3. **Create a dictionary.** I mapped each word to a single number, in a single dictionary.
4. **Represent each line as a bag-of-words.** Turn each line into a vector of the same length as the dictionary, with a number for every time that a word occurs in the sentence (since the dictionary is much more vast than the sentence, the vector will mostly be composed of zeroes).
5. **Calculate the [TF-IDF score](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) for each word.** The TF-IDF score represents how important a word is in the corpus overall. It is higher for words that appear many times in the document, and lower for words that appear a lot in the entire corpus.
6. **Generate a similarity index.** Gensim creates a semantic index based on cosine similarity that can be queried very efficiently.  Thi index is a large file, which gets split into several shards that must be loaded into memory for a new query.

The most concise form of the code looks like this:

```python
import nltk
import pandas as pd
from nltk.tokenize import sent_tokenize, word_tokenize

df2 = pd.read_csv('simpsons_script_lines.csv',error_bad_lines=False)
df2['token'] = [word_tokenize(str(x)) for x in df2['normalized_text']]
dictionary = gensim.corpora.Dictionary(df2['token'])
corpus = [dictionary.doc2bow(row) for row in df2['token']]
tf_idf = gensim.models.TfidfModel(corpus)
similarity_index = gensim.similarities.Similarity('/shards/shard',tf_idf[corpus],
                                      num_features=len(dictionary))
```
And that's it!  New queries from the user are processed in the same manner as all the original lines of dialog, and queried against the similarity index.  The result is an array of how similar the query is to *every single line of dialog ever*.  We query the original dataframe for the top 10 most similar indices in that array, and thus get the top 10 quotes that most closely match the query.

```python
query_tokenized = [w.lower() for w in word_tokenize(query)]
query_bow = dictionary.doc2bow(query_tokenized)
query_tfidf = tf_idf[query_bow]
array_of_similarities = similarity_index[query_tfidf]
top_10_indices = array_of_similarities.argsort()[-10:][::-1]
top_10_lines = df[df.index.isin(result)]
```
We wrote up a web app to return raw results that look somethin like this:

```
[{"quote_id":13144,"raw_character_text":"Bart Simpson","spoken_words":"Hey, Homer, I can't find the safety goggles for the power saw.","episode_title":"Saturdays of Thunder","season":3,"number_in_season":9},{"quote_id":37513,"raw_character_text":"Radioactive Man","spoken_words":"My eyes! The goggles do nothing.","episode_title":"Radioactive Man","season":7,"number_in_season":2},{"quote_id":64870,"raw_character_text":"Milhouse Van Houten","spoken_words":"My sport goggles!","episode_title":"Brother's Little Helper","season":11,"number_in_season":2},{"quote_id":78425,"raw_character_text":"Marge Simpson","spoken_words":"I want goggles, too.","episode_title":"The Parent Rap","season":13,"number_in_season":2},{"quote_id":79442,"raw_character_text":"Space Ship Captain","spoken_words":"Right. Goggles on!","episode_title":"She of Little Faith","season":13,"number_in_season":6}]
```

Our team of web developers turned this into a website where the results look more like this:

![Squid query](/img/8_simpson/baltic1.png)

![Squid results](/img/8_simpson/baltic2.png)

[You can try it yourself here](https://simpsonssays.netlify.com/), in a full website that also allows users to log in and save favorite quotes. Note that, because we are using a free tier of Heroku to deploy the app, the website takes about 2 minutes to load results when it hasn't been accessed for a while.

# Using a recurrent neural network to generate new text.
My colleage adapted the TensorFlow code from [this blog post](https://towardsdatascience.com/how-to-generate-your-own-the-simpsons-tv-script-using-deep-learning-980337173796) to build a recurrent neural network (RNN), train it with dialog lines from specific characters, and generate synthetic quotes that we also fed into our website. This work was carried out by my collaborator, and can be found in [this Jupyter Notebook](https://github.com/simpson-says/buildweek3-simpsons-says-ds/blob/master/Simpsons_Writes_V4.ipynb).

We trained the RNN on dialog lines from several characters, then collected the results into a single dictionary with ~100 quotes per character (after removing really short quotes because they don't really capture any specific style).  Here are some sample synthetic lines for Marge Simpson:

```
(reading) you can't have a lot of mittens this is
(indignant) you promised to know what about the gun?
(angry) bart was where we ever just?
lisa! i just think you're spoiling them.
```

# Deploying the model to the web
In order to make the results of our models available online, we created a little web application using [Flask](http://flask.pocoo.org/) that our web developers could connect to.  The application itself is deployed on the cloud platform Heroku, and responds to user POST requests with JSON objects containing the results of our model.  




You can try out the barebones web application [here](https://eat-my-shorts.herokuapp.com/), and imagine how our team of web developers transformed that into [the full website](https://simpsonssays.netlify.com/).  You can find the full code of our web application [here](https://github.com/simpson-says/buildweek3-simpsons-says-ds/blob/master/app.py).