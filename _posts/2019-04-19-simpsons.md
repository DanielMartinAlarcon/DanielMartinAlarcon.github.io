---
title: Simpson Says
subtitle: Scripts from The Simpsons as a basis for Natural Language Processing
image: /img/2_population-bubbles/bubbles-cropped.png
---

**TL;DR**: I used the script from ~600 episodes of The Simpsons as the basis for some neat natural language processing demos. This was a one-week project with web developers at Lambda School, and resulted in [a live website](https://simpsonssays.netlify.com/) where you can access our models yourself.  We wrote a search function to find existing dialog lines based on text similarity, and a recurrent neural network to generate new quotes in the style of particular characters.  The full repo for this project is [here](https://github.com/simpson-says/buildweek3-simpsons-says-ds).

# Using text similarity to find iconic quotes.
The full script for about 27 seasons and 600 episodes of the show are available [on Kaggle](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data). The most important tools we used here are the Natural Language Toolkit ([NLTK](https://www.nltk.org/)) and [Gensim](https://radimrehurek.com/gensim/). 

Relatively few steps are needed to make this work:  

1. **Clean the text.**  I made a pandas dataframe where the column `normalized_text` contains all the lines of dialog for all the episodes, stripped of punctuation and turned to lowercase.  
2. **Tokenize.** Each row was a string with a single line of dialog.  I split that string into a list of individual words. 
3. **Create a dictionary.** I mapped each word to a single number, in a single dictionary.
4. **Represent each line as a bag-of-words.** Turn each line into a vector of the same length as the dictionary, with a number for every time that a word occurs in the sentence (since the dictionary is much more vast than the sentence, the vector will mostly be composed of zeroes).
5. **Calculate the [TF-IDF score](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) for each word.** The TF-IDF score represents how important a word is in the corpus overall. It is higher for words that appear many times in the document, and lower for words that appear a lot in the entire corpus.
6. **Generate a similarity index.** Gensim creates an index based on cosine similarity that can be queried very efficiently.  It stores the index in shards of predetermined size.

```python
import nltk
import pandas as pd
from nltk.tokenize import sent_tokenize, word_tokenize

df2 = pd.read_csv('simpsons_script_lines.csv',error_bad_lines=False)
df2['token'] = [word_tokenize(str(x)) for x in df2['normalized_text']]
dictionary = gensim.corpora.Dictionary(df2['token'])
corpus = [dictionary.doc2bow(row) for row in df2['token']]
tf_idf = gensim.models.TfidfModel(corpus)
similarity_index = gensim.similarities.Similarity('/shards/shard',tf_idf[corpus],
                                      num_features=len(dictionary))
```
And that's it!  New queries from the user are processed in the same manner as all the original lines of dialog, and queried against the similarity index.  The result is an array of how similar the query is to *every single line of dialog ever*.  We query the original dataframe for the top 10 most similar indices in that array, and thus get the top 10 quotes that most closely match the query.

```python
query_tokenized = [w.lower() for w in word_tokenize(query)]
query_bow = dictionary.doc2bow(query_tokenized)
query_tfidf = tf_idf[query_bow]
array_of_similarities = similarity_index[query_tfidf]
top_10_indices = array_of_similarities.argsort()[-10:][::-1]
top_10_lines = df[df.index.isin(result)]
```

This is what the results look like:

![Squid query](/img/8_simpson/baltic1.png)

![Squid results](/img/8_simpson/baltic2.png)

[You can try it yourself here](https://simpsonssays.netlify.com/), but note that it takes a minute to load when the website hasn't been accessed for a while.

# Using a recurrent neural network to generate new text.
We adapted the TensorFlow code from [this blog post](https://towardsdatascience.com/how-to-generate-your-own-the-simpsons-tv-script-using-deep-learning-980337173796) to build a recurrent neural network (RNN), train it with dialog lines from specific characters, and generate synthetic quotes that we also fed into our website. This work was carried out by my collaborator, and can be found in [this Jupyter Notebook](https://github.com/simpson-says/buildweek3-simpsons-says-ds/blob/master/Simpsons_Writes_V4.ipynb).

We trained the RNN on dialog lines from several characters, then collected the results into a single dictionary with ~100 quotes per character (after removing really short quotes because they don't really capture any specific style).  Here are some sample synthetic lines for Marge Simpson:

```
(reading) you can't have a lot of mittens this is
(indignant) you promised to know what about the gun?
(angry) bart was where we ever just?
lisa! i just think you're spoiling them.
```

# Deploying the model to the web
In order to make the results of our models available on the web, we created a little web application using [Flask](http://flask.pocoo.org/) that our web developers could connect to.  The application itself is deployed on the cloud platform Heroku, and responds to user POST requests with JSON objects containing the results of our model.  You can see the barebones web application [here](https://eat-my-shorts.herokuapp.com/), and imagine how web developers turned that into [the full website](https://simpsonssays.netlify.com/).  You can find the full code of our web application [here](https://github.com/simpson-says/buildweek3-simpsons-says-ds/blob/master/app.py).